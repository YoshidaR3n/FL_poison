{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25bd5a6-a975-4113-adac-83f64ed1af8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda using PyTorch 2.2.0+cu121 and Flower.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "from flwr_datasets import FederatedDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower.\"\n",
    ")\n",
    "disable_progress_bar()\n",
    "\n",
    "\n",
    "args = sys.argv\n",
    "ID = args[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb82c8cd-b54b-41f7-85dd-707172a72b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "#クロスサイロ化するためにデータを分割\n",
    "############################################################\n",
    "\n",
    "NUM_CLIENTS = 3\n",
    "BATCH_SIZE = 32\n",
    "POISON = True\n",
    "\n",
    "def load_datasets():\n",
    "    fds = FederatedDataset(dataset=\"cifar10\", partitioners={\"train\": NUM_CLIENTS})\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ]\n",
    "        )\n",
    "        batch[\"img\"] = [transform(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    # Create train/val for each partition and wrap it into DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "\n",
    "    for partition_id in range(NUM_CLIENTS):\n",
    "        partition = fds.load_partition(partition_id, \"train\")\n",
    "        partition = partition.with_transform(apply_transforms)\n",
    "        partition = partition.train_test_split(train_size=0.8)\n",
    "        \n",
    "        if(partition_id==2 and POISON==True):\n",
    "            for data in partition[\"train\"]:\n",
    "                if data[\"label\"] == 3:\n",
    "                    data[\"label\"] = 4\n",
    "                \n",
    "        trainloaders.append(DataLoader(partition[\"train\"], batch_size=BATCH_SIZE))\n",
    "        valloaders.append(DataLoader(partition[\"test\"], batch_size=BATCH_SIZE))\n",
    "\n",
    "    testset = fds.load_full(\"test\").with_transform(apply_transforms)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7d076-7746-43df-ab52-0dff03317c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#モデルの定義・学習(連合)\n",
    "############################################################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \"\"\"\n",
    "        self.conv1=nn.Conv2d(3,64,3) #32-3=29\n",
    "        self.conv2=nn.Conv2d(64,128,3) #29-3=26\n",
    "        self.pool1=nn.MaxPool2d(2,2) #26/2=13\n",
    "        self.conv3=nn.Conv2d(128,256,3) #13-3=10\n",
    "        self.conv4=nn.Conv2d(256,512,2) #10-2=8\n",
    "        self.pool2=nn.MaxPool2d(2,2) #8/2=4\n",
    "        self.fc1=nn.Linear(512 * 4 * 4, 1024)\n",
    "        self.fc2=nn.Linear(1024, 256)\n",
    "        self.fc3=nn.Linear(256, 10)\n",
    "        self.relu=F.relu\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def train(net, trainloader, epochs: int, verbose=True):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "\n",
    "    print(f\"一つのデータセットの大きさ{len(trainloaders[0])}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for batch in trainloader:\n",
    "            images, labels = batch[\"img\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images, labels = batch[\"img\"].to(DEVICE), batch[\"label\"].to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n",
    "\n",
    "trainloader = trainloaders[int(ID)]\n",
    "valloader = valloaders[int(ID)]\n",
    "net = Net().to(DEVICE)\n",
    "\n",
    "############################################################\n",
    "#連合学習\n",
    "############################################################\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    #print(\"receve:\")\n",
    "    #print(parameters[0][0][0][0])\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    #print(\"send:\")\n",
    "    #a=[val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "    #print(a[0][0][0][0])\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, net, trainloader, valloader):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "    \"\"\"\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(net, parameters)\n",
    "        train(net, trainloader, epochs=5)\n",
    "        loss, accuracy = test(net, valloader)\n",
    "        print(f\"ClientID:{ID} validation loss {round(loss,4)}, accuracy {round(accuracy,4)}\")\n",
    "        return get_parameters(net), len(trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(net, parameters)\n",
    "        loss, accuracy = test(net, valloader)\n",
    "        return float(loss), len(valloader), {\"accuracy\": float(accuracy)}\n",
    "    \n",
    "############################################################\n",
    "#接続\n",
    "############################################################\n",
    "fl.client.start_numpy_client(server_address=\"127.0.0.1:8080\", client=FlowerClient())\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myFL",
   "language": "python",
   "name": "myfl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
